{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from scipy.stats import wilcoxon, norm\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, ParameterSampler, HalvingRandomSearchCV, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score, balanced_accuracy_score, matthews_corrcoef, cohen_kappa_score,\n",
    "    make_scorer, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    ")\n",
    "\n",
    "rng = np.random.RandomState(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c20dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes | X: (581012, 58) | y: (581012,)\n",
      "\n",
      "dtypes head:\n",
      " Elevation                               int64\n",
      "Aspect                                  int64\n",
      "Facet                                 float64\n",
      "Slope                                 float64\n",
      "Inclination                           float64\n",
      "Horizontal_Distance_To_Hydrology        int64\n",
      "Vertical_Distance_To_Hydrology          int64\n",
      "Horizontal_Distance_To_Roadways         int64\n",
      "Hillshade_9am                           int64\n",
      "Hillshade_Noon                          int64\n",
      "Hillshade_3pm                           int64\n",
      "Horizontal_Distance_To_Fire_Points      int64\n",
      "dtype: object\n",
      "\n",
      "Class distribution:\n",
      "             count    pct\n",
      "Cover_Type               \n",
      "1           211840  36.46\n",
      "2           283301  48.76\n",
      "3            35754   6.15\n",
      "4             2747   0.47\n",
      "5             9493   1.63\n",
      "6            17367   2.99\n",
      "7            20510   3.53\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"forestCover.csv\", na_values=[\"?\"])\n",
    "df = df.rename(columns={\"Water_Level\": \"Observation_ID\", \"Observation_ID\": \"Water_Level\"})\n",
    "\n",
    "TARGET_COL = \"Cover_Type\"\n",
    "\n",
    "y = df[TARGET_COL].copy()\n",
    "X = df.drop(columns=[TARGET_COL]).copy()\n",
    "\n",
    "print(f\"Shapes | X: {X.shape} | y: {y.shape}\")\n",
    "print(\"\\ndtypes head:\\n\", X.dtypes.head(12))\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(y.value_counts().sort_index().to_frame(\"count\").assign(pct=lambda t: 100*t[\"count\"]/len(y)).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58139c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric: 14  |  Binary: 44  |  Non-binary categoricals: 0\n",
      "Numeric:\n",
      "  Elevation, Aspect, Facet, Slope\n",
      "  Inclination, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways\n",
      "  Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points\n",
      "  Observation_ID, Water_Level\n",
      "Binary:\n",
      "  Wilderness_Area1, Wilderness_Area2, Wilderness_Area3, Wilderness_Area4\n",
      "  Soil_Type1, Soil_Type2, Soil_Type3, Soil_Type4\n",
      "  Soil_Type5, Soil_Type6, Soil_Type7, Soil_Type8\n",
      "  Soil_Type9, Soil_Type10, Soil_Type11, Soil_Type12\n",
      "  Soil_Type13, Soil_Type14, Soil_Type15, Soil_Type16\n",
      "  Soil_Type17, Soil_Type18, Soil_Type19, Soil_Type20\n",
      "  Soil_Type21, Soil_Type22, Soil_Type23, Soil_Type24\n",
      "  Soil_Type25, Soil_Type26, Soil_Type27, Soil_Type28\n",
      "  Soil_Type29, Soil_Type30, Soil_Type31, Soil_Type32\n",
      "  Soil_Type33, Soil_Type34, Soil_Type35, Soil_Type36\n",
      "  Soil_Type37, Soil_Type38, Soil_Type39, Soil_Type40\n",
      "Categorical:  []\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "\n",
    "TRUE_TOKENS  = {\"1\",\"true\",\"t\",\"yes\",\"y\",\"pos\",\"positive\",\"on\"}\n",
    "FALSE_TOKENS = {\"0\",\"false\",\"f\",\"no\",\"n\",\"neg\",\"negative\",\"off\"}\n",
    "\n",
    "def is_semantic_binary(values: pd.Series) -> bool:\n",
    "    #Returns True if it looks like a binary field\n",
    "    uniq = pd.unique(values.dropna().astype(str).str.strip().str.lower())\n",
    "    if len(uniq) != 2:\n",
    "        return False\n",
    "    a, b = uniq\n",
    "    \n",
    "    return ((a in TRUE_TOKENS and b in FALSE_TOKENS) or (a in FALSE_TOKENS and b in TRUE_TOKENS))\n",
    "\n",
    "def infer_feature_types(frame: pd.DataFrame) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Return (numeric_cols, binary_indicator_cols, non_binary_categoricals)\n",
    "    \"\"\"\n",
    "    num_cols, bin_cols, cat_cols = [], [], []\n",
    "    for c in frame.columns:\n",
    "        s = frame[c]\n",
    "        # Fast path for numeric dtypes\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            uniq = pd.unique(s.dropna())\n",
    "            if len(uniq) == 2 and set(pd.to_numeric(uniq, errors=\"coerce\")) <= {0,1}:\n",
    "                bin_cols.append(c)\n",
    "            else:\n",
    "                num_cols.append(c)\n",
    "            continue\n",
    "\n",
    "        # Object/string-like: check binary semantics first\n",
    "        if is_semantic_binary(s):\n",
    "            bin_cols.append(c)\n",
    "        else:\n",
    "            # If exactly two string levels but not recognized, still treat as binary\n",
    "            uniq = pd.unique(s.dropna())\n",
    "            if len(uniq) == 2:\n",
    "                bin_cols.append(c)\n",
    "            else:\n",
    "                cat_cols.append(c)\n",
    "    return num_cols, bin_cols, cat_cols\n",
    "\n",
    "NUM_COLS, BIN_COLS, CAT_COLS = infer_feature_types(X)\n",
    "print(f\"Numeric: {len(NUM_COLS)}  |  Binary: {len(BIN_COLS)}  |  Non-binary categoricals: {len(CAT_COLS)}\")\n",
    "print(\"Numeric:\")\n",
    "for i in range(0, len(NUM_COLS), 4):\n",
    "    print(\"  \" + \", \".join(NUM_COLS[i:i+4]))\n",
    "print(\"Binary:\")\n",
    "for i in range(0, len(BIN_COLS), 4):\n",
    "    print(\"  \" + \", \".join(BIN_COLS[i:i+4]))\n",
    "print(\"Categorical: \", CAT_COLS)\n",
    "if CAT_COLS:\n",
    "    print(\"Non-binary categoricals (unexpected here, just flagging):\", CAT_COLS[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ace66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA AUDIT SUMMARY\n",
      "Rows: 581,012 | Features: 58 (numeric=14, binary=44)\n",
      "Missing rows (any missing): 298\n",
      "Columns with missing values: 1\n",
      "Slope    298\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows: 0\n",
      "Zero-variance columns: 1\n",
      "['Water_Level']\n",
      "\n",
      "Near-zero-variance numeric: 1\n",
      "['Water_Level']\n",
      "\n",
      "Top numeric columns with robust outliers (>|3.5| z):\n",
      "Vertical_Distance_To_Hydrology        57894\n",
      "Horizontal_Distance_To_Fire_Points    41988\n",
      "Hillshade_9am                         30893\n",
      "Horizontal_Distance_To_Hydrology      29620\n",
      "Elevation                             27133\n",
      "Hillshade_Noon                        24203\n",
      "Horizontal_Distance_To_Roadways       22566\n",
      "Slope                                 18962\n",
      "Hillshade_3pm                         14852\n",
      "dtype: int64\n",
      "\n",
      "High-correlation numeric pairs @|r|≥0.95: 1\n",
      "  Aspect ~ Facet |r|=1.000\n",
      "\n",
      "Class distribution:\n",
      "              count    pct\n",
      "Cover_Type               \n",
      "1           211840  36.46\n",
      "2           283301  48.76\n",
      "3            35754   6.15\n",
      "4             2747   0.47\n",
      "5             9493   1.63\n",
      "6            17367   2.99\n",
      "7            20510   3.53\n",
      "Imbalance max/min ratio: 103.13\n",
      "Class entropy (bits): 1.739\n",
      "\n",
      "Low-cardinality (nunique ≤ 1): ['Water_Level']\n",
      "Potential ID-like columns (≈ unique per row): ['Facet', 'Inclination', 'Observation_ID']\n",
      "Sequential/monotone step-constant columns: ['Observation_ID', 'Water_Level']\n"
     ]
    }
   ],
   "source": [
    "#Check the data to confirm the data quality issues mentioned in the assignment\n",
    "def audit_dataset(X: pd.DataFrame, y: pd.Series, num_cols, bin_cols, corr_threshold=0.95):\n",
    "    report = {}\n",
    "\n",
    "    # Missingness\n",
    "    miss_per_col = X.isna().sum()\n",
    "    miss_rows = X.isna().any(axis=1).sum()\n",
    "    report[\"missing_cols_nonzero\"] = miss_per_col[miss_per_col > 0].sort_values(ascending=False)\n",
    "    report[\"missing_rows_nonzero\"] = miss_rows\n",
    "\n",
    "    # Duplicates\n",
    "    dup_rows = X.duplicated().sum()\n",
    "    report[\"duplicate_rows\"] = dup_rows\n",
    "\n",
    "    # Zero and near-zero variance (binary and numeric separately)\n",
    "    def nzv_mask(series, tol=1e-12):\n",
    "        # Treat as zero-variance if all values equal; only compute std for numeric dtypes\n",
    "        if series.nunique(dropna=False) == 1:\n",
    "            return True\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            return series.std(skipna=True) < tol\n",
    "        return False\n",
    "\n",
    "    zero_var_cols = [c for c in X.columns if nzv_mask(X[c])]\n",
    "    report[\"zero_variance_cols\"] = zero_var_cols\n",
    "\n",
    "    # Near-zero variance heuristic for numeric only (IQR ~ 0)\n",
    "    nzv_numeric = []\n",
    "    for c in num_cols:\n",
    "        q1, q3 = X[c].quantile([0.25, 0.75])\n",
    "        if pd.isna(q1) or pd.isna(q3):\n",
    "            continue\n",
    "        if np.isclose(q3 - q1, 0.0):\n",
    "            nzv_numeric.append(c)\n",
    "    report[\"near_zero_variance_numeric\"] = nzv_numeric\n",
    "\n",
    "    # Scale & distribution diagnostics (numeric only)\n",
    "    desc = X[num_cols].describe().T if num_cols else pd.DataFrame()\n",
    "    if not desc.empty:\n",
    "        desc[\"iqr\"] = desc[\"75%\"] - desc[\"25%\"]\n",
    "        # Robust z-score count beyond |3.5|\n",
    "        med = X[num_cols].median()\n",
    "        mad = (X[num_cols] - med).abs().median().replace(0, np.nan)\n",
    "        robust_z = (X[num_cols] - med).div(mad).abs()\n",
    "        outlier_counts = (robust_z > 3.5).sum().sort_values(ascending=False)\n",
    "        report[\"outlier_counts_numeric\"] = outlier_counts[outlier_counts > 0]\n",
    "        report[\"numeric_summary\"] = desc[[\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\", \"iqr\"]]\n",
    "\n",
    "    # Correlation structure (numeric only)\n",
    "    high_corr_pairs = []\n",
    "    if len(num_cols) >= 2:\n",
    "        corr = X[num_cols].corr().abs()\n",
    "        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "        hits = np.where((upper >= corr_threshold))\n",
    "        for i, j in zip(*hits):\n",
    "            high_corr_pairs.append((num_cols[i], num_cols[j], upper.iloc[i, j]))\n",
    "    report[\"high_corr_pairs\"] = sorted(high_corr_pairs, key=lambda t: t[2], reverse=True)\n",
    "\n",
    "    # Cardinality of each feature\n",
    "    cardinality = X.nunique(dropna=True).sort_values()\n",
    "    report[\"cardinality\"] = cardinality\n",
    "\n",
    "    # Potential ID-like columns: nearly all values unique (≥95% of rows unique)\n",
    "    id_like_cols = list(cardinality[cardinality >= 0.95*len(X)].index)\n",
    "    report[\"id_like_cols\"] = id_like_cols\n",
    "\n",
    "    # Sequential/monotone with constant step (e.g., 2,3,4,...): likely row index in disguise\n",
    "    seq_like_cols = []\n",
    "    for c in X.columns:\n",
    "        s = X[c]\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            nonna = s.dropna()\n",
    "            if len(nonna) > 2 and nonna.is_monotonic_increasing:\n",
    "                diffs = nonna.diff().dropna()\n",
    "                if diffs.nunique() == 1:\n",
    "                    seq_like_cols.append(c)\n",
    "    report[\"sequential_like_cols\"] = seq_like_cols\n",
    "\n",
    "    # Class distribution + imbalance summary\n",
    "    counts = y.value_counts().sort_index()\n",
    "    pct = (100 * counts / len(y)).round(2)\n",
    "    max_min_ratio = (counts.max() / counts.min()) if counts.min() > 0 else np.inf\n",
    "    # Entropy (base-2)\n",
    "    p = counts / counts.sum()\n",
    "    entropy = -(p * np.log2(p)).sum()\n",
    "    report[\"class_distribution\"] = pd.DataFrame({\"count\": counts, \"pct\": pct})\n",
    "    report[\"imbalance_max_min_ratio\"] = max_min_ratio\n",
    "    report[\"class_entropy_bits\"] = float(entropy)\n",
    "\n",
    "    print(\"DATA AUDIT SUMMARY\")\n",
    "    print(f\"Rows: {len(X):,} | Features: {X.shape[1]} (numeric={len(num_cols)}, binary={len(bin_cols)})\")\n",
    "    print(f\"Missing rows (any missing): {report['missing_rows_nonzero']:,}\")\n",
    "    print(f\"Columns with missing values: {len(report['missing_cols_nonzero'])}\")\n",
    "    if len(report[\"missing_cols_nonzero\"]) > 0:\n",
    "        print(report[\"missing_cols_nonzero\"].head(10))\n",
    "\n",
    "    print(f\"\\nDuplicate rows: {report['duplicate_rows']:,}\")\n",
    "    print(f\"Zero-variance columns: {len(report['zero_variance_cols'])}\")\n",
    "    if report[\"zero_variance_cols\"][:10]:\n",
    "        print(report[\"zero_variance_cols\"][:10])\n",
    "\n",
    "    print(f\"\\nNear-zero-variance numeric: {len(report['near_zero_variance_numeric'])}\")\n",
    "    if report[\"near_zero_variance_numeric\"][:10]:\n",
    "        print(report[\"near_zero_variance_numeric\"][:10])\n",
    "\n",
    "    if \"outlier_counts_numeric\" in report and not report[\"outlier_counts_numeric\"].empty:\n",
    "        print(\"\\nTop numeric columns with robust outliers (>|3.5| z):\")\n",
    "        print(report[\"outlier_counts_numeric\"].head(10))\n",
    "\n",
    "    print(f\"\\nHigh-correlation numeric pairs @|r|≥0.95: {len(report['high_corr_pairs'])}\")\n",
    "    for trip in report[\"high_corr_pairs\"][:5]:\n",
    "        print(f\"  {trip[0]} ~ {trip[1]} |r|={trip[2]:.3f}\")\n",
    "\n",
    "    print(\"\\nClass distribution:\\n\", report[\"class_distribution\"])\n",
    "    print(f\"Imbalance max/min ratio: {report['imbalance_max_min_ratio']:.2f}\")\n",
    "    print(f\"Class entropy (bits): {report['class_entropy_bits']:.3f}\")\n",
    "\n",
    "    low_card = list(cardinality[cardinality <= 1].index)\n",
    "    print(f\"\\nLow-cardinality (nunique ≤ 1): {low_card}\")\n",
    "    if id_like_cols:\n",
    "        print(f\"Potential ID-like columns (≈ unique per row): {id_like_cols[:8]}\")\n",
    "    if seq_like_cols:\n",
    "        print(f\"Sequential/monotone step-constant columns: {seq_like_cols[:8]}\")\n",
    "\n",
    "\n",
    "    return report\n",
    "\n",
    "audit = audit_dataset(X, y, NUM_COLS, BIN_COLS, corr_threshold=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30139285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['Facet', 'Water_Level', 'Observation_ID', 'Inclination']\n"
     ]
    }
   ],
   "source": [
    "# Drop features due to data quality issues from assignment\n",
    "to_drop = []\n",
    "\n",
    "# Perfect collinearity: keep Aspect, drop Facet\n",
    "if \"Aspect\" in X.columns and \"Facet\" in X.columns:\n",
    "    to_drop.append(\"Facet\")\n",
    "\n",
    "# Zero-variance columns\n",
    "zero_var = [c for c, k in X.nunique(dropna=True).items() if k <= 1]\n",
    "to_drop += zero_var\n",
    "\n",
    "# Observation_ID\n",
    "if \"Observation_ID\" in X.columns:\n",
    "    nunq = X[\"Observation_ID\"].nunique(dropna=True)\n",
    "    if nunq <= 1 or nunq >= 0.95*len(X):\n",
    "        to_drop.append(\"Observation_ID\")\n",
    "    else:\n",
    "        s = X[\"Observation_ID\"].dropna()\n",
    "        if len(s) > 2 and s.is_monotonic_increasing and s.diff().dropna().nunique() == 1:\n",
    "            to_drop.append(\"Observation_ID\")\n",
    "\n",
    "# Pure noise feature\n",
    "DROP_NOISY_INCLINATION = True\n",
    "if DROP_NOISY_INCLINATION and \"Inclination\" in X.columns:\n",
    "    to_drop.append(\"Inclination\")\n",
    "\n",
    "# Apply drops\n",
    "to_drop = [c for c in dict.fromkeys(to_drop) if c in X.columns]\n",
    "X = X.drop(columns=to_drop).copy()\n",
    "print(\"Dropped columns:\", to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "393ce5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary string→{0,1} mappings: {'Soil_Type1': {'positive': 1, 'negative': 0}}\n"
     ]
    }
   ],
   "source": [
    "TRUE_TOKENS  = {\"1\",\"true\",\"t\",\"yes\",\"y\",\"pos\",\"positive\",\"on\"}\n",
    "FALSE_TOKENS = {\"0\",\"false\",\"f\",\"no\",\"n\",\"neg\",\"negative\",\"off\"}\n",
    "\n",
    "def is_semantic_binary(s):\n",
    "    u = pd.unique(s.dropna().astype(str).str.strip().str.lower())\n",
    "    return len(u) == 2 and ((u[0] in TRUE_TOKENS and u[1] in FALSE_TOKENS) or\n",
    "                            (u[0] in FALSE_TOKENS and u[1] in TRUE_TOKENS))\n",
    "\n",
    "def mapping_for_binary(s):\n",
    "    u = pd.unique(s.dropna().astype(str).str.strip())\n",
    "    low = {v: v.lower() for v in u}\n",
    "    a, b = u[0], u[1]\n",
    "    if low[a] in TRUE_TOKENS and low[b] in FALSE_TOKENS: return {a:1, b:0}\n",
    "    if low[a] in FALSE_TOKENS and low[b] in TRUE_TOKENS: return {a:0, b:1}\n",
    "    # deterministic fallback\n",
    "    a1, b1 = sorted([(a,low[a]),(b,low[b])], key=lambda t:t[1])\n",
    "    return {b1[0]:1, a1[0]:0}\n",
    "\n",
    "binary_string_cols = [c for c in X.columns if X[c].dtype=='object' and is_semantic_binary(X[c])]\n",
    "binary_mappings = {}\n",
    "for c in binary_string_cols:\n",
    "    m = mapping_for_binary(X[c]); X[c] = X[c].map(m).astype(\"Int8\"); binary_mappings[c]=m\n",
    "\n",
    "print(\"Binary string→{0,1} mappings:\", {k: v for k,v in list(binary_mappings.items())[:5]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b78f29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fixed split] numeric: 10 | binary: 44\n",
      "Sample numerics: ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon']\n",
      "Sample binaries: ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4']\n"
     ]
    }
   ],
   "source": [
    "for c in X.columns:\n",
    "    s = X[c]\n",
    "    if is_numeric_dtype(s):\n",
    "        vals = pd.unique(s.dropna())\n",
    "        if len(vals) > 0 and set(map(float, vals)) <= {0.0, 1.0}:\n",
    "            X[c] = s.astype('int8', copy=False)\n",
    "\n",
    "def split_num_bin(df: pd.DataFrame):\n",
    "    num_cols, bin_cols = [], []\n",
    "    for col in df.columns:\n",
    "        s = df[col]\n",
    "        if is_numeric_dtype(s):\n",
    "            vals = pd.unique(s.dropna())\n",
    "            if len(vals) > 0 and set(map(float, vals)) <= {0.0, 1.0}:\n",
    "                bin_cols.append(col)\n",
    "            else:\n",
    "                num_cols.append(col)\n",
    "        else:\n",
    "            if s.dropna().nunique() == 2:\n",
    "                bin_cols.append(col)\n",
    "    return num_cols, bin_cols\n",
    "\n",
    "NUM_COLS, BIN_COLS = split_num_bin(X)\n",
    "print(f\"[Fixed split] numeric: {len(NUM_COLS)} | binary: {len(BIN_COLS)}\")\n",
    "print(\"Sample numerics:\", NUM_COLS[:8])\n",
    "print(\"Sample binaries:\", BIN_COLS[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cols = [c for c in X.columns if np.issubdtype(X[c].dtype, np.number) and set(pd.unique(X[c].dropna())) - {0,1}]\n",
    "bin_cols = [c for c in X.columns if np.issubdtype(X[c].dtype, np.number) and set(pd.unique(X[c].dropna())) <= {0,1}]\n",
    "\n",
    "has_missing = X.isna().any().any()\n",
    "\n",
    "knn_pre = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "                      (\"scale\", RobustScaler(quantile_range=(25,75)))]), num_cols),\n",
    "    (\"bin\", Pipeline([(\"impute\", SimpleImputer(strategy=\"most_frequent\"))]), bin_cols),\n",
    "], verbose_feature_names_out=False)\n",
    "\n",
    "knn_clf = Pipeline([(\"prep\", knn_pre), (\"clf\", KNeighborsClassifier())])\n",
    "\n",
    "tree_pre = (\"passthrough\" if not has_missing else\n",
    "            ColumnTransformer([\n",
    "                (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
    "                (\"bin\", SimpleImputer(strategy=\"most_frequent\"), bin_cols),\n",
    "            ], verbose_feature_names_out=False))\n",
    "\n",
    "tree_clf = Pipeline([(\"prep\", tree_pre), (\"clf\", DecisionTreeClassifier(random_state=42))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76405248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline k-NN (defaults) ===\n",
      "                mean     std\n",
      "macro_f1      0.8810  0.0024\n",
      "balanced_acc  0.8698  0.0030\n",
      "kappa         0.8901  0.0017\n",
      "mcc           0.8902  0.0016\n",
      "\n",
      "=== Baseline Decision Tree (defaults) ===\n",
      "                mean     std\n",
      "macro_f1      0.9060  0.0034\n",
      "balanced_acc  0.9049  0.0033\n",
      "kappa         0.9079  0.0015\n",
      "mcc           0.9079  0.0015\n"
     ]
    }
   ],
   "source": [
    "baseline_scoring = {\n",
    "    \"macro_f1\": make_scorer(f1_score, average=\"macro\"),\n",
    "    \"balanced_acc\": make_scorer(balanced_accuracy_score),\n",
    "    \"kappa\": make_scorer(cohen_kappa_score),\n",
    "    \"mcc\": make_scorer(matthews_corrcoef),\n",
    "}\n",
    "baseline_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=2025)\n",
    "\n",
    "def summarize_cv(cvres: dict) -> pd.DataFrame:\n",
    "    return (\n",
    "        pd.DataFrame({m: [np.mean(cvres[f\"test_{m}\"]), np.std(cvres[f\"test_{m}\"])] for m in baseline_scoring})\n",
    "          .T.rename(columns={0:\"mean\", 1:\"std\"}).round(4)\n",
    "    )\n",
    "\n",
    "print(\"=== Baseline k-NN (defaults) ===\")\n",
    "knn_base = cross_validate(knn_clf, X, y, cv=baseline_cv, scoring=baseline_scoring, n_jobs=-1, return_train_score=False)\n",
    "print(summarize_cv(knn_base))\n",
    "\n",
    "print(\"\\n=== Baseline Decision Tree (defaults) ===\")\n",
    "tree_base = cross_validate(tree_clf, X, y, cv=baseline_cv, scoring=baseline_scoring, n_jobs=-1, return_train_score=False)\n",
    "print(summarize_cv(tree_base))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ffdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Nested CV [k-NN] | outer=10, inner=5 | method=HalvingRandomSearchCV ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 440 is smaller than n_iter=7470. Running 440 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 01 | best params -> {'clf__weights': 'distance', 'clf__p': 2, 'clf__n_neighbors': 1, 'clf__metric': 'minkowski'}\n",
      "  Test macro-F1: 0.8881 | BalAcc: 0.8911 | κ: 0.8996 | MCC: 0.8996 | Train acc: 1.0000\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19900   1175     1    0   17     4    87\n",
      "2   1162  26858    79    0  141    79    12\n",
      "3      2     62  3271   40   13   187     0\n",
      "4      0      0    33  224    0    18     0\n",
      "5     28    133    12    0  770     4     2\n",
      "6      3     65   148   20    9  1492     0\n",
      "7     90     10     0    0    1     0  1950\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9394), np.int64(2): np.float64(0.9485), np.int64(3): np.float64(0.9189), np.int64(4): np.float64(0.8014), np.int64(5): np.float64(0.8105), np.int64(6): np.float64(0.8475), np.int64(7): np.float64(0.9508)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 440 is smaller than n_iter=7470. Running 440 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 02 | best params -> {'clf__weights': 'distance', 'clf__p': 1, 'clf__n_neighbors': 4, 'clf__metric': 'minkowski'}\n",
      "  Test macro-F1: 0.8974 | BalAcc: 0.8909 | κ: 0.9074 | MCC: 0.9074 | Train acc: 1.0000\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19930   1138     2    0   25     1    88\n",
      "2    992  27071    58    0  111    78    20\n",
      "3      1     69  3307   18    9   172     0\n",
      "4      0      0    43  217    0    15     0\n",
      "5     20    153     8    0  762     5     1\n",
      "6      3     56   143   13    3  1519     0\n",
      "7     91     15     0    0    0     0  1945\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9441), np.int64(2): np.float64(0.9527), np.int64(3): np.float64(0.9267), np.int64(4): np.float64(0.8298), np.int64(5): np.float64(0.8198), np.int64(6): np.float64(0.8614), np.int64(7): np.float64(0.9476)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 440 is smaller than n_iter=7470. Running 440 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 03 | best params -> {'clf__weights': 'distance', 'clf__p': 1, 'clf__n_neighbors': 4, 'clf__metric': 'minkowski'}\n",
      "  Test macro-F1: 0.9033 | BalAcc: 0.8985 | κ: 0.9092 | MCC: 0.9092 | Train acc: 1.0000\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19900   1174     1    0   12     2    95\n",
      "2    951  27139    77    1  108    46     8\n",
      "3      3     69  3311   23   10   160     0\n",
      "4      1      0    37  225    0    11     0\n",
      "5     12    130    16    0  787     5     0\n",
      "6      3     57   160   10    7  1499     0\n",
      "7     85     11     0    0    0     0  1955\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9445), np.int64(2): np.float64(0.9538), np.int64(3): np.float64(0.9225), np.int64(4): np.float64(0.8443), np.int64(5): np.float64(0.8399), np.int64(6): np.float64(0.8667), np.int64(7): np.float64(0.9516)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 440 is smaller than n_iter=7470. Running 440 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 04 | best params -> {'clf__weights': 'uniform', 'clf__p': 1, 'clf__n_neighbors': 1, 'clf__metric': 'minkowski'}\n",
      "  Test macro-F1: 0.8886 | BalAcc: 0.8846 | κ: 0.9000 | MCC: 0.9000 | Train acc: 1.0000\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19898   1167     2    0   21     3    93\n",
      "2   1156  26903    63    1  113    78    16\n",
      "3      0     73  3291   36   10   166     0\n",
      "4      0      0    46  211    0    17     0\n",
      "5     17    129     7    0  791     5     1\n",
      "6      6     75   187   11    4  1453     0\n",
      "7     93     22     0    0    1     0  1935\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9396), np.int64(2): np.float64(0.949), np.int64(3): np.float64(0.9177), np.int64(4): np.float64(0.7917), np.int64(5): np.float64(0.837), np.int64(6): np.float64(0.8404), np.int64(7): np.float64(0.9448)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 440 is smaller than n_iter=7470. Running 440 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 05 | best params -> {'clf__weights': 'distance', 'clf__p': 1, 'clf__n_neighbors': 4, 'clf__metric': 'minkowski'}\n",
      "  Test macro-F1: 0.9014 | BalAcc: 0.8961 | κ: 0.9087 | MCC: 0.9087 | Train acc: 1.0000\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19972   1131     0    0   15     4    62\n",
      "2    986  27062    90    0  103    78    11\n",
      "3      1     81  3305   26    6   157     0\n",
      "4      0      0    41  220    0    13     0\n",
      "5     16    130     6    0  793     5     0\n",
      "6      3     74   150   12    5  1492     0\n",
      "7     87      8     0    0    0     0  1956\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9454), np.int64(2): np.float64(0.9526), np.int64(3): np.float64(0.9222), np.int64(4): np.float64(0.8271), np.int64(5): np.float64(0.8472), np.int64(6): np.float64(0.8562), np.int64(7): np.float64(0.9588)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 440 is smaller than n_iter=7470. Running 440 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 06 | best params -> {'clf__weights': 'uniform', 'clf__p': 1, 'clf__n_neighbors': 1, 'clf__metric': 'minkowski'}\n",
      "  Test macro-F1: 0.8925 | BalAcc: 0.8886 | κ: 0.8989 | MCC: 0.8989 | Train acc: 1.0000\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19859   1210     4    0   17     7    87\n",
      "2   1159  26861    89    0  128    80    13\n",
      "3      1     76  3302   24    6   166     0\n",
      "4      0      0    33  218    0    24     0\n",
      "5     24    147     2    0  773     3     0\n",
      "6      2     64   156   15    7  1493     0\n",
      "7     95     18     0    0    1     0  1937\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9384), np.int64(2): np.float64(0.9474), np.int64(3): np.float64(0.9222), np.int64(4): np.float64(0.8195), np.int64(5): np.float64(0.8219), np.int64(6): np.float64(0.8507), np.int64(7): np.float64(0.9477)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 440 is smaller than n_iter=7470. Running 440 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 07 | best params -> {'clf__weights': 'distance', 'clf__p': 1, 'clf__n_neighbors': 6, 'clf__metric': 'minkowski'}\n",
      "  Test macro-F1: 0.8993 | BalAcc: 0.8908 | κ: 0.9037 | MCC: 0.9037 | Train acc: 1.0000\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19841   1215     0    0   20    10    98\n",
      "2   1022  27049    68    1   85    91    14\n",
      "3      4     80  3317   21    4   149     0\n",
      "4      0      0    38  224    0    13     0\n",
      "5     19    154    16    0  757     3     0\n",
      "6      6     78   156    9    0  1488     0\n",
      "7     93     15     0    0    0     0  1943\n",
      "  Per-class F1: {np.int64(1): np.float64(0.941), np.int64(2): np.float64(0.9504), np.int64(3): np.float64(0.9252), np.int64(4): np.float64(0.8453), np.int64(5): np.float64(0.8342), np.int64(6): np.float64(0.8525), np.int64(7): np.float64(0.9464)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 440 is smaller than n_iter=7470. Running 440 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 08 | best params -> {'clf__weights': 'distance', 'clf__p': 1, 'clf__n_neighbors': 1, 'clf__metric': 'minkowski'}\n",
      "  Test macro-F1: 0.8958 | BalAcc: 0.8946 | κ: 0.9025 | MCC: 0.9025 | Train acc: 1.0000\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19922   1139     1    0   19     6    97\n",
      "2   1173  26890    62    1  122    67    15\n",
      "3      5     67  3304   28    5   166     0\n",
      "4      0      0    32  220    0    23     0\n",
      "5     24    112     8    0  799     6     0\n",
      "6      5     76   147   17    6  1486     0\n",
      "7     93      7     0    0    0     0  1951\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9396), np.int64(2): np.float64(0.9498), np.int64(3): np.float64(0.9269), np.int64(4): np.float64(0.8133), np.int64(5): np.float64(0.8411), np.int64(6): np.float64(0.8513), np.int64(7): np.float64(0.9485)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 440 is smaller than n_iter=7470. Running 440 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 09 | best params -> {'clf__weights': 'distance', 'clf__p': 1, 'clf__n_neighbors': 4, 'clf__metric': 'minkowski'}\n",
      "  Test macro-F1: 0.9004 | BalAcc: 0.8923 | κ: 0.9056 | MCC: 0.9057 | Train acc: 1.0000\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19882   1192     1    0   15     5    89\n",
      "2   1016  27079    74    0   88    63    10\n",
      "3      2     61  3309   26    6   171     0\n",
      "4      0      0    40  222    0    13     0\n",
      "5     15    140     6    0  781     7     0\n",
      "6      2     82   169    6    2  1476     0\n",
      "7     89     19     0    0    2     0  1941\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9425), np.int64(2): np.float64(0.9518), np.int64(3): np.float64(0.9225), np.int64(4): np.float64(0.8393), np.int64(5): np.float64(0.8475), np.int64(6): np.float64(0.8502), np.int64(7): np.float64(0.9489)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 440 is smaller than n_iter=7470. Running 440 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10 | best params -> {'clf__weights': 'distance', 'clf__p': 1, 'clf__n_neighbors': 3, 'clf__metric': 'minkowski'}\n",
      "  Test macro-F1: 0.8996 | BalAcc: 0.8957 | κ: 0.9030 | MCC: 0.9030 | Train acc: 1.0000\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19852   1229     4    0   20     5    74\n",
      "2   1080  26999    71    0  100    70    10\n",
      "3      0     84  3283   30    9   169     0\n",
      "4      0      1    33  226    0    15     0\n",
      "5     23    129    10    0  783     4     0\n",
      "6      6     74   140   12    3  1502     0\n",
      "7     94      9     0    0    0     0  1948\n",
      "  Per-class F1: {np.int64(1): np.float64(0.94), np.int64(2): np.float64(0.9497), np.int64(3): np.float64(0.9227), np.int64(4): np.float64(0.8324), np.int64(5): np.float64(0.8401), np.int64(6): np.float64(0.8578), np.int64(7): np.float64(0.9542)}\n",
      "\n",
      "=== Outer-fold summary [k-NN] (mean ± std) ===\n",
      "      macro_f1  accuracy  balanced_acc   kappa     mcc  macro_precision  macro_recall  fit_time_s  \\\n",
      "mean    0.8966    0.9401        0.8923  0.9039  0.9039           0.9013        0.8923    110.5330   \n",
      "std     0.0053    0.0024        0.0041  0.0038  0.0038           0.0080        0.0041     22.4978   \n",
      "\n",
      "      pred_time_per_sample_ms  train_acc  \n",
      "mean                   0.9459        1.0  \n",
      "std                    0.2474        0.0  \n",
      "\n",
      "Per-class F1 (mean over folds): {np.int64(1): np.float64(0.9415), np.int64(2): np.float64(0.9506), np.int64(3): np.float64(0.9228), np.int64(4): np.float64(0.8244), np.int64(5): np.float64(0.8339), np.int64(6): np.float64(0.8535), np.int64(7): np.float64(0.9499)}\n",
      "\n",
      "=== Nested CV [Decision Tree] | outer=10, inner=5 | method=HalvingRandomSearchCV ===\n",
      "Fold 01 | best params -> {'clf__min_samples_split': 2, 'clf__min_samples_leaf': 1, 'clf__max_features': np.float64(0.8), 'clf__max_depth': 40, 'clf__class_weight': None, 'clf__ccp_alpha': np.float64(0.0)}\n",
      "  Test macro-F1: 0.9062 | BalAcc: 0.9056 | κ: 0.9089 | MCC: 0.9089 | Train acc: 1.0000\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19985   1084     0    0   20     4    91\n",
      "2   1139  26936    68    1  121    55    11\n",
      "3      2     68  3343   27   11   124     0\n",
      "4      0      1    36  226    0    12     0\n",
      "5     18    122    11    0  794     3     1\n",
      "6      4     50   111   14    4  1554     0\n",
      "7     74     14     0    0    0     0  1963\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9426), np.int64(2): np.float64(0.9517), np.int64(3): np.float64(0.9359), np.int64(4): np.float64(0.8324), np.int64(5): np.float64(0.8362), np.int64(6): np.float64(0.8908), np.int64(7): np.float64(0.9536)}\n",
      "Fold 02 | best params -> {'clf__min_samples_split': 5, 'clf__min_samples_leaf': 1, 'clf__max_features': None, 'clf__max_depth': None, 'clf__class_weight': 'balanced', 'clf__ccp_alpha': np.float64(0.0)}\n",
      "  Test macro-F1: 0.8994 | BalAcc: 0.9039 | κ: 0.9018 | MCC: 0.9018 | Train acc: 0.9928\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19926   1122     3    0   18     6   109\n",
      "2   1175  26826    87    1  138    87    16\n",
      "3      1     62  3319   33    9   152     0\n",
      "4      0      0    27  238    0    10     0\n",
      "5     14    127     6    0  793     6     3\n",
      "6      3     62   141   14    1  1516     0\n",
      "7    117     10     0    0    0     0  1924\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9395), np.int64(2): np.float64(0.9489), np.int64(3): np.float64(0.9272), np.int64(4): np.float64(0.8485), np.int64(5): np.float64(0.8312), np.int64(6): np.float64(0.8628), np.int64(7): np.float64(0.9379)}\n",
      "Fold 03 | best params -> {'clf__min_samples_split': 2, 'clf__min_samples_leaf': 2, 'clf__max_features': np.float64(0.6), 'clf__max_depth': 29, 'clf__class_weight': None, 'clf__ccp_alpha': np.float64(0.0)}\n",
      "  Test macro-F1: 0.8974 | BalAcc: 0.8839 | κ: 0.8982 | MCC: 0.8983 | Train acc: 0.9843\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  20025   1058     1    0   14     3    83\n",
      "2   1382  26729    71    0   98    40    10\n",
      "3      1     88  3348   22    8   109     0\n",
      "4      0      0    43  226    0     5     0\n",
      "5     30    172    17    0  727     3     1\n",
      "6      2     83   160    9    4  1478     0\n",
      "7    141     21     0    0    0     0  1889\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9365), np.int64(2): np.float64(0.9465), np.int64(3): np.float64(0.9279), np.int64(4): np.float64(0.8512), np.int64(5): np.float64(0.8073), np.int64(6): np.float64(0.8761), np.int64(7): np.float64(0.9365)}\n",
      "Fold 04 | best params -> {'clf__min_samples_split': 2, 'clf__min_samples_leaf': 2, 'clf__max_features': np.float64(0.6), 'clf__max_depth': 29, 'clf__class_weight': None, 'clf__ccp_alpha': np.float64(0.0)}\n",
      "  Test macro-F1: 0.8839 | BalAcc: 0.8705 | κ: 0.8884 | MCC: 0.8884 | Train acc: 0.9825\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19841   1259     0    0   16     3    65\n",
      "2   1456  26641    77    0  108    39     9\n",
      "3      2    112  3289   38    8   127     0\n",
      "4      0      0    47  210    0    17     0\n",
      "5     22    196     9    0  720     3     0\n",
      "6      5     76   173    8    5  1469     0\n",
      "7    128     22     0    0    1     0  1900\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9307), np.int64(2): np.float64(0.9408), np.int64(3): np.float64(0.9173), np.int64(4): np.float64(0.7925), np.int64(5): np.float64(0.7965), np.int64(6): np.float64(0.8656), np.int64(7): np.float64(0.9441)}\n",
      "Fold 05 | best params -> {'clf__min_samples_split': 5, 'clf__min_samples_leaf': 2, 'clf__max_features': np.float64(1.0), 'clf__max_depth': None, 'clf__class_weight': 'balanced', 'clf__ccp_alpha': np.float64(0.0)}\n",
      "  Test macro-F1: 0.8983 | BalAcc: 0.9110 | κ: 0.9003 | MCC: 0.9004 | Train acc: 0.9862\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  20045    980     2    0   20     3   134\n",
      "2   1345  26598   139    1  136    99    12\n",
      "3      3     65  3289   44   19   156     0\n",
      "4      0      0    27  239    0     8     0\n",
      "5     13    103     9    0  823     2     0\n",
      "6      5     56   131   15    5  1524     0\n",
      "7     81     10     0    0    0     0  1960\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9394), np.int64(2): np.float64(0.9475), np.int64(3): np.float64(0.9171), np.int64(4): np.float64(0.8342), np.int64(5): np.float64(0.8428), np.int64(6): np.float64(0.8639), np.int64(7): np.float64(0.943)}\n",
      "Fold 06 | best params -> {'clf__min_samples_split': 2, 'clf__min_samples_leaf': 2, 'clf__max_features': np.float64(0.7), 'clf__max_depth': 29, 'clf__class_weight': None, 'clf__ccp_alpha': np.float64(0.0)}\n",
      "  Test macro-F1: 0.8978 | BalAcc: 0.8853 | κ: 0.8906 | MCC: 0.8907 | Train acc: 0.9825\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19969   1122     2    0    7     2    82\n",
      "2   1603  26507    68    2   95    48     7\n",
      "3      2    108  3343   23    6    93     0\n",
      "4      0      2    35  230    0     8     0\n",
      "5     24    176     5    0  742     2     0\n",
      "6      6     85   164   12    3  1467     0\n",
      "7    133     28     0    0    1     0  1889\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9305), np.int64(2): np.float64(0.9407), np.int64(3): np.float64(0.9296), np.int64(4): np.float64(0.8487), np.int64(5): np.float64(0.8231), np.int64(6): np.float64(0.874), np.int64(7): np.float64(0.9377)}\n",
      "Fold 07 | best params -> {'clf__min_samples_split': 20, 'clf__min_samples_leaf': 1, 'clf__max_features': np.float64(0.9), 'clf__max_depth': 37, 'clf__class_weight': None, 'clf__ccp_alpha': np.float64(0.0)}\n",
      "  Test macro-F1: 0.8919 | BalAcc: 0.8853 | κ: 0.8855 | MCC: 0.8855 | Train acc: 0.9688\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19726   1312     1    0   19     3   123\n",
      "2   1441  26575    92    1  119    79    23\n",
      "3      7     88  3304   18   10   148     0\n",
      "4      0      0    33  231    0    11     0\n",
      "5     16    167    16    0  747     2     1\n",
      "6     10     77   144    9    8  1489     0\n",
      "7    153     12     0    0    0     0  1886\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9275), np.int64(2): np.float64(0.9397), np.int64(3): np.float64(0.9223), np.int64(4): np.float64(0.8652), np.int64(5): np.float64(0.8067), np.int64(6): np.float64(0.8585), np.int64(7): np.float64(0.9236)}\n",
      "Fold 08 | best params -> {'clf__min_samples_split': 2, 'clf__min_samples_leaf': 1, 'clf__max_features': np.float64(0.8), 'clf__max_depth': 40, 'clf__class_weight': None, 'clf__ccp_alpha': np.float64(0.0)}\n",
      "  Test macro-F1: 0.9032 | BalAcc: 0.9022 | κ: 0.9091 | MCC: 0.9091 | Train acc: 1.0000\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  20024   1050     0    0   15     7    88\n",
      "2   1106  26954    76    0  125    56    13\n",
      "3      2     74  3335   33    7   124     0\n",
      "4      0      0    37  227    0    11     0\n",
      "5     21    131     7    0  784     6     0\n",
      "6      3     64   125   13    5  1527     0\n",
      "7     84      8     0    0    0     0  1959\n",
      "  Per-class F1: {np.int64(1): np.float64(0.944), np.int64(2): np.float64(0.9523), np.int64(3): np.float64(0.9322), np.int64(4): np.float64(0.8285), np.int64(5): np.float64(0.8318), np.int64(6): np.float64(0.8806), np.int64(7): np.float64(0.9531)}\n",
      "Fold 09 | best params -> {'clf__min_samples_split': 20, 'clf__min_samples_leaf': 2, 'clf__max_features': np.float64(0.7), 'clf__max_depth': 28, 'clf__class_weight': None, 'clf__ccp_alpha': np.float64(0.0)}\n",
      "  Test macro-F1: 0.8790 | BalAcc: 0.8725 | κ: 0.8750 | MCC: 0.8750 | Train acc: 0.9610\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19580   1448     1    0   23     6   126\n",
      "2   1569  26462    99    0  124    63    13\n",
      "3      1     98  3257   32    3   184     0\n",
      "4      0      0    35  226    0    14     0\n",
      "5     22    184    10    0  728     5     0\n",
      "6      5     96   180   12    0  1444     0\n",
      "7    141     26     0    0    1     0  1883\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9214), np.int64(2): np.float64(0.9343), np.int64(3): np.float64(0.9102), np.int64(4): np.float64(0.8294), np.int64(5): np.float64(0.7965), np.int64(6): np.float64(0.8364), np.int64(7): np.float64(0.9246)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10 | best params -> {'clf__min_samples_split': 5, 'clf__min_samples_leaf': 5, 'clf__max_features': np.float64(0.6), 'clf__max_depth': 35, 'clf__class_weight': None, 'clf__ccp_alpha': np.float64(0.0)}\n",
      "  Test macro-F1: 0.8797 | BalAcc: 0.8711 | κ: 0.8799 | MCC: 0.8799 | Train acc: 0.9676\n",
      "  Confusion matrix (rows=true, cols=pred):\n",
      "        1      2     3    4    5     6     7\n",
      "1  19609   1442     5    0   18     6   104\n",
      "2   1479  26575    91    0  111    55    19\n",
      "3      8     93  3279   40   11   144     0\n",
      "4      0      1    39  216    0    19     0\n",
      "5     25    169    16    0  733     6     0\n",
      "6      7    103   176    9    2  1440     0\n",
      "7    126     17     0    0    0     0  1908\n",
      "  Per-class F1: {np.int64(1): np.float64(0.9241), np.int64(2): np.float64(0.9369), np.int64(3): np.float64(0.9132), np.int64(4): np.float64(0.8), np.int64(5): np.float64(0.8037), np.int64(6): np.float64(0.8453), np.int64(7): np.float64(0.9348)}\n",
      "\n",
      "=== Outer-fold summary [Decision Tree] (mean ± std) ===\n",
      "      macro_f1  accuracy  balanced_acc   kappa     mcc  macro_precision  macro_recall  fit_time_s  \\\n",
      "mean    0.8937    0.9338        0.8892  0.8938  0.8938           0.8988        0.8892   1147.9650   \n",
      "std     0.0097    0.0073        0.0154  0.0117  0.0117           0.0098        0.0154    327.5814   \n",
      "\n",
      "      pred_time_per_sample_ms  train_acc  \n",
      "mean                   0.0005     0.9826  \n",
      "std                    0.0000     0.0134  \n",
      "\n",
      "Per-class F1 (mean over folds): {np.int64(1): np.float64(0.9336), np.int64(2): np.float64(0.9439), np.int64(3): np.float64(0.9233), np.int64(4): np.float64(0.833), np.int64(5): np.float64(0.8176), np.int64(6): np.float64(0.8654), np.int64(7): np.float64(0.9389)}\n",
      "\n",
      "=== Wilcoxon signed-rank (macro-F1, k-NN minus Tree) ===\n",
      "Statistic=20.000, p-value=0.4922, effect size r=0.217\n",
      "Median(kNN-Tree) diff: 0.0039\n",
      "Conclusion: No significant difference at α=0.05.\n",
      "\n",
      "=== Table A draft (mean/std over outer folds) ===\n",
      "                    macro_f1  accuracy  balanced_acc   kappa     mcc  macro_precision  macro_recall  fit_time_s  \\\n",
      "k-NN          mean    0.8966    0.9401        0.8923  0.9039  0.9039           0.9013        0.8923    110.5330   \n",
      "              std     0.0053    0.0024        0.0041  0.0038  0.0038           0.0080        0.0041     22.4978   \n",
      "Decision Tree mean    0.8937    0.9338        0.8892  0.8938  0.8938           0.8988        0.8892   1147.9650   \n",
      "              std     0.0097    0.0073        0.0154  0.0117  0.0117           0.0098        0.0154    327.5814   \n",
      "\n",
      "                    pred_time_per_sample_ms  train_acc  \n",
      "k-NN          mean                   0.9459     1.0000  \n",
      "              std                    0.2474     0.0000  \n",
      "Decision Tree mean                   0.0005     0.9826  \n",
      "              std                    0.0000     0.0134  \n",
      "\n",
      "=== Table C draft: best params per outer fold ===\n",
      "   fold                                           kNN_best                                          Tree_best\n",
      "0     1  {'clf__weights': 'distance', 'clf__p': 2, 'clf...  {'clf__min_samples_split': 2, 'clf__min_sample...\n",
      "1     2  {'clf__weights': 'distance', 'clf__p': 1, 'clf...  {'clf__min_samples_split': 5, 'clf__min_sample...\n",
      "2     3  {'clf__weights': 'distance', 'clf__p': 1, 'clf...  {'clf__min_samples_split': 2, 'clf__min_sample...\n",
      "3     4  {'clf__weights': 'uniform', 'clf__p': 1, 'clf_...  {'clf__min_samples_split': 2, 'clf__min_sample...\n",
      "4     5  {'clf__weights': 'distance', 'clf__p': 1, 'clf...  {'clf__min_samples_split': 5, 'clf__min_sample...\n",
      "5     6  {'clf__weights': 'uniform', 'clf__p': 1, 'clf_...  {'clf__min_samples_split': 2, 'clf__min_sample...\n",
      "6     7  {'clf__weights': 'distance', 'clf__p': 1, 'clf...  {'clf__min_samples_split': 20, 'clf__min_sampl...\n",
      "7     8  {'clf__weights': 'distance', 'clf__p': 1, 'clf...  {'clf__min_samples_split': 2, 'clf__min_sample...\n",
      "8     9  {'clf__weights': 'distance', 'clf__p': 1, 'clf...  {'clf__min_samples_split': 20, 'clf__min_sampl...\n",
      "9    10  {'clf__weights': 'distance', 'clf__p': 1, 'clf...  {'clf__min_samples_split': 5, 'clf__min_sample...\n"
     ]
    }
   ],
   "source": [
    "# Empirical Procedure: 10x5 nested CV with halving, metrics, logging, Wilcoxon \n",
    "scoring = {\n",
    "    \"macro_f1\": make_scorer(f1_score, average=\"macro\"),\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"balanced_acc\": make_scorer(balanced_accuracy_score),\n",
    "    \"kappa\": make_scorer(cohen_kappa_score),\n",
    "    \"mcc\": make_scorer(matthews_corrcoef),\n",
    "    \"macro_precision\": make_scorer(precision_score, average=\"macro\", zero_division=0),\n",
    "    \"macro_recall\": make_scorer(recall_score, average=\"macro\", zero_division=0),\n",
    "}\n",
    "primary_key = \"macro_f1\"\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=2025)\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "class_labels = np.sort(y.unique())\n",
    "\n",
    "knn_space = {\n",
    "    \"clf__n_neighbors\": list(range(1, 56)),\n",
    "    \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "    \"clf__metric\": [\"minkowski\", \"chebyshev\"],\n",
    "    \"clf__p\": [1, 2],\n",
    "}\n",
    "\n",
    "tree_space = {\n",
    "    \"clf__max_depth\": [None] + list(range(3, 41)),\n",
    "    \"clf__min_samples_leaf\": [1, 2, 5, 10, 20, 50],\n",
    "    \"clf__min_samples_split\": [2, 5, 10, 20, 50],\n",
    "    \"clf__max_features\": [None, \"sqrt\", \"log2\"] + list(np.round(np.linspace(0.2, 1.0, 9), 2)),\n",
    "    \"clf__ccp_alpha\": np.linspace(0.0, 0.02, 21),\n",
    "    \"clf__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "def sample_candidates(space, n_candidates, rng):\n",
    "    return list(ParameterSampler(space, n_iter=n_candidates, random_state=rng))\n",
    "\n",
    "def nested_run(label, pipe, space, n_candidates=200, use_halving=True, factor=3, outer_splits=None):\n",
    "    print(f\"\\n=== Nested CV [{label}] | outer=10, inner=5 | method={'HalvingRandomSearchCV' if use_halving else 'RandomizedSearchCV'} ===\")\n",
    "    outer_recs = []\n",
    "    best_params_per_fold = []\n",
    "    per_fold_confmats = []\n",
    "    per_fold_perclassF1 = []\n",
    "\n",
    "    candidates = sample_candidates(space, n_candidates=n_candidates, rng=rng)\n",
    "\n",
    "    for fold_idx, (tr, te) in enumerate(outer_splits, 1):\n",
    "        X_tr, X_te = X.iloc[tr], X.iloc[te]\n",
    "        y_tr, y_te = y.iloc[tr], y.iloc[te]\n",
    "\n",
    "        if use_halving:\n",
    "            search = HalvingRandomSearchCV(\n",
    "                estimator=pipe,\n",
    "                param_distributions=space,\n",
    "                resource=\"n_samples\",\n",
    "                factor=factor,\n",
    "                scoring=\"f1_macro\",\n",
    "                cv=inner_cv,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                verbose=0\n",
    "            )\n",
    "        else:\n",
    "            search = RandomizedSearchCV(\n",
    "                estimator=pipe,\n",
    "                param_distributions=space,\n",
    "                n_iter=n_candidates,\n",
    "                scoring=scoring,\n",
    "                refit=primary_key,\n",
    "                cv=inner_cv,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        search.fit(X_tr, y_tr)\n",
    "        fit_time = time.perf_counter() - t0\n",
    "\n",
    "        best_params_per_fold.append(search.best_params_)\n",
    "\n",
    "        y_tr_pred = search.best_estimator_.predict(X_tr)\n",
    "        train_acc = accuracy_score(y_tr, y_tr_pred)\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        y_pred = search.best_estimator_.predict(X_te)\n",
    "        pred_time = time.perf_counter() - t1\n",
    "        pred_time_per_sample = pred_time / len(X_te)\n",
    "\n",
    "        rec = {\n",
    "            \"fold\": fold_idx,\n",
    "            \"macro_f1\": f1_score(y_te, y_pred, average=\"macro\"),\n",
    "            \"accuracy\": accuracy_score(y_te, y_pred),\n",
    "            \"balanced_acc\": balanced_accuracy_score(y_te, y_pred),\n",
    "            \"kappa\": cohen_kappa_score(y_te, y_pred),\n",
    "            \"mcc\": matthews_corrcoef(y_te, y_pred),\n",
    "            \"macro_precision\": precision_score(y_te, y_pred, average=\"macro\", zero_division=0),\n",
    "            \"macro_recall\": recall_score(y_te, y_pred, average=\"macro\", zero_division=0),\n",
    "            \"fit_time_s\": fit_time,\n",
    "            \"pred_time_per_sample_ms\": 1000 * pred_time_per_sample,\n",
    "            \"train_acc\": train_acc,\n",
    "        }\n",
    "        outer_recs.append(rec)\n",
    "\n",
    "        cm = confusion_matrix(y_te, y_pred, labels=class_labels)\n",
    "        per_fold_confmats.append(cm)\n",
    "        per_class_f1 = f1_score(y_te, y_pred, average=None, labels=class_labels)\n",
    "        per_fold_perclassF1.append(per_class_f1)\n",
    "\n",
    "        print(f\"Fold {fold_idx:02d} | best params -> {search.best_params_}\")\n",
    "        print(\"  Test macro-F1: {:.4f} | BalAcc: {:.4f} | κ: {:.4f} | MCC: {:.4f} | Train acc: {:.4f}\".format(\n",
    "            rec[\"macro_f1\"], rec[\"balanced_acc\"], rec[\"kappa\"], rec[\"mcc\"], train_acc\n",
    "        ))\n",
    "        print(\"  Confusion matrix (rows=true, cols=pred):\\n\", pd.DataFrame(cm, index=class_labels, columns=class_labels))\n",
    "        print(\"  Per-class F1:\", dict(zip(class_labels, np.round(per_class_f1, 4))))\n",
    "\n",
    "    df_outer = pd.DataFrame(outer_recs)\n",
    "    summary = df_outer.describe().loc[[\"mean\", \"std\"]].round(4)\n",
    "    print(\"\\n=== Outer-fold summary [{}] (mean ± std) ===\".format(label))\n",
    "    print(summary[[\"macro_f1\",\"accuracy\",\"balanced_acc\",\"kappa\",\"mcc\",\"macro_precision\",\"macro_recall\",\"fit_time_s\",\"pred_time_per_sample_ms\",\"train_acc\"]])\n",
    "\n",
    "    agg_cm = np.sum(np.stack(per_fold_confmats, axis=0), axis=0)\n",
    "    agg_cm_df = pd.DataFrame(agg_cm, index=class_labels, columns=class_labels)\n",
    "\n",
    "    per_class_f1_mean = np.mean(np.vstack(per_fold_perclassF1), axis=0)\n",
    "    print(\"\\nPer-class F1 (mean over folds):\", dict(zip(class_labels, np.round(per_class_f1_mean, 4))))\n",
    "\n",
    "    return df_outer, best_params_per_fold, agg_cm_df, per_class_f1_mean\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=2025)\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "\n",
    "outer_splits = list(outer_cv.split(X, y))\n",
    "knn_results, knn_best, knn_cm, knn_pcF1 = nested_run(\"k-NN\", knn_clf, knn_space, n_candidates=200, use_halving=True, factor=3, outer_splits=outer_splits)\n",
    "tree_results, tree_best, tree_cm, tree_pcF1 = nested_run(\"Decision Tree\", tree_clf, tree_space, n_candidates=300, use_halving=True, factor=3, outer_splits=outer_splits)\n",
    "\n",
    "knn_scores = knn_results.sort_values(\"fold\")[\"macro_f1\"].to_numpy()\n",
    "tree_scores = tree_results.sort_values(\"fold\")[\"macro_f1\"].to_numpy()\n",
    "diffs = knn_scores - tree_scores\n",
    "\n",
    "stat, p = wilcoxon(knn_scores, tree_scores, alternative=\"two-sided\", zero_method=\"wilcox\")\n",
    "z = norm.isf(p/2.0) * np.sign(np.mean(diffs))\n",
    "r = z / np.sqrt(len(diffs))\n",
    "median_diff = np.median(diffs)\n",
    "\n",
    "print(\"\\n=== Wilcoxon signed-rank (macro-F1, k-NN minus Tree) ===\")\n",
    "print(f\"Statistic={stat:.3f}, p-value={p:.4g}, effect size r={r:.3f}\")\n",
    "print(f\"Median(kNN-Tree) diff: {median_diff:.4f}\")\n",
    "print(\"Conclusion:\", \"Significant difference at α=0.05.\" if p < 0.05 else \"No significant difference at α=0.05.\")\n",
    "\n",
    "def fold_summary_table(df, label):\n",
    "    cols = [\"macro_f1\",\"accuracy\",\"balanced_acc\",\"kappa\",\"mcc\",\"macro_precision\",\"macro_recall\",\"fit_time_s\",\"pred_time_per_sample_ms\",\"train_acc\"]\n",
    "    s = df.describe().loc[[\"mean\",\"std\"], cols].round(4)\n",
    "    s.index = pd.MultiIndex.from_product([[label], s.index])\n",
    "    return s\n",
    "\n",
    "report_table = pd.concat([fold_summary_table(knn_results, \"k-NN\"),\n",
    "                          fold_summary_table(tree_results, \"Decision Tree\")])\n",
    "\n",
    "print(\"\\n=== Table A draft (mean/std over outer folds) ===\")\n",
    "print(report_table)\n",
    "\n",
    "print(\"\\n=== Table C draft: best params per outer fold ===\")\n",
    "print(pd.DataFrame({\"fold\": range(1, len(knn_best)+1),\n",
    "                    \"kNN_best\": knn_best,\n",
    "                    \"Tree_best\": tree_best}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.width\", 300)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e4c96b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Table C draft: best params per outer fold ===\n",
      "   fold                                                                                      kNN_best                                                                                                                                                             Tree_best\n",
      "0     1  {'clf__weights': 'distance', 'clf__p': 2, 'clf__n_neighbors': 1, 'clf__metric': 'minkowski'}           {'clf__min_samples_split': 2, 'clf__min_samples_leaf': 1, 'clf__max_features': 0.8, 'clf__max_depth': 40, 'clf__class_weight': None, 'clf__ccp_alpha': 0.0}\n",
      "1     2  {'clf__weights': 'distance', 'clf__p': 1, 'clf__n_neighbors': 4, 'clf__metric': 'minkowski'}  {'clf__min_samples_split': 5, 'clf__min_samples_leaf': 1, 'clf__max_features': None, 'clf__max_depth': None, 'clf__class_weight': 'balanced', 'clf__ccp_alpha': 0.0}\n",
      "2     3  {'clf__weights': 'distance', 'clf__p': 1, 'clf__n_neighbors': 4, 'clf__metric': 'minkowski'}           {'clf__min_samples_split': 2, 'clf__min_samples_leaf': 2, 'clf__max_features': 0.6, 'clf__max_depth': 29, 'clf__class_weight': None, 'clf__ccp_alpha': 0.0}\n",
      "3     4   {'clf__weights': 'uniform', 'clf__p': 1, 'clf__n_neighbors': 1, 'clf__metric': 'minkowski'}           {'clf__min_samples_split': 2, 'clf__min_samples_leaf': 2, 'clf__max_features': 0.6, 'clf__max_depth': 29, 'clf__class_weight': None, 'clf__ccp_alpha': 0.0}\n",
      "4     5  {'clf__weights': 'distance', 'clf__p': 1, 'clf__n_neighbors': 4, 'clf__metric': 'minkowski'}   {'clf__min_samples_split': 5, 'clf__min_samples_leaf': 2, 'clf__max_features': 1.0, 'clf__max_depth': None, 'clf__class_weight': 'balanced', 'clf__ccp_alpha': 0.0}\n",
      "5     6   {'clf__weights': 'uniform', 'clf__p': 1, 'clf__n_neighbors': 1, 'clf__metric': 'minkowski'}           {'clf__min_samples_split': 2, 'clf__min_samples_leaf': 2, 'clf__max_features': 0.7, 'clf__max_depth': 29, 'clf__class_weight': None, 'clf__ccp_alpha': 0.0}\n",
      "6     7  {'clf__weights': 'distance', 'clf__p': 1, 'clf__n_neighbors': 6, 'clf__metric': 'minkowski'}          {'clf__min_samples_split': 20, 'clf__min_samples_leaf': 1, 'clf__max_features': 0.9, 'clf__max_depth': 37, 'clf__class_weight': None, 'clf__ccp_alpha': 0.0}\n",
      "7     8  {'clf__weights': 'distance', 'clf__p': 1, 'clf__n_neighbors': 1, 'clf__metric': 'minkowski'}           {'clf__min_samples_split': 2, 'clf__min_samples_leaf': 1, 'clf__max_features': 0.8, 'clf__max_depth': 40, 'clf__class_weight': None, 'clf__ccp_alpha': 0.0}\n",
      "8     9  {'clf__weights': 'distance', 'clf__p': 1, 'clf__n_neighbors': 4, 'clf__metric': 'minkowski'}          {'clf__min_samples_split': 20, 'clf__min_samples_leaf': 2, 'clf__max_features': 0.7, 'clf__max_depth': 28, 'clf__class_weight': None, 'clf__ccp_alpha': 0.0}\n",
      "9    10  {'clf__weights': 'distance', 'clf__p': 1, 'clf__n_neighbors': 3, 'clf__metric': 'minkowski'}           {'clf__min_samples_split': 5, 'clf__min_samples_leaf': 5, 'clf__max_features': 0.6, 'clf__max_depth': 35, 'clf__class_weight': None, 'clf__ccp_alpha': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Table C draft: best params per outer fold ===\")\n",
    "print(pd.DataFrame({\"fold\": range(1, len(knn_best)+1),\n",
    "                    \"kNN_best\": knn_best,\n",
    "                    \"Tree_best\": tree_best}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c16c875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/np/k3jsjm6n5xn7qyzm8sr6j2tr0000gn/T/ipykernel_78596/3696295677.py:30: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  bp = ax.boxplot([knn_f1, tree_f1], labels=[\"k-NN\", \"Decision Tree\"], showmeans=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: figures2/boxplot_macroF1_kNN_vs_Tree.png\n",
      "Saved: figures2/paired_macroF1_per_fold.png\n",
      "Saved: figures2/confusion_kNN_rownorm_annotated.png\n",
      "Saved: figures2/confusion_kNN_misclass_only.png\n",
      "Saved: figures2/confusion_Tree_rownorm_annotated.png\n",
      "Saved: figures2/confusion_Tree_misclass_only.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "outdir = Path(\"./figures\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_close(fig, name):\n",
    "    path = outdir / name\n",
    "    fig.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\"Saved:\", path)\n",
    "\n",
    "knn_f1 = knn_results.sort_values(\"fold\")[\"macro_f1\"].to_numpy()\n",
    "tree_f1 = tree_results.sort_values(\"fold\")[\"macro_f1\"].to_numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(5.4, 4.6))\n",
    "ax = plt.gca()\n",
    "bp = ax.boxplot([knn_f1, tree_f1], labels=[\"k-NN\", \"Decision Tree\"], showmeans=True)\n",
    "\n",
    "x_jitter = 0.06\n",
    "x_pos = np.array([1, 2])\n",
    "ax.scatter(np.full_like(knn_f1, x_pos[0]) + (np.random.rand(len(knn_f1))-0.5)*x_jitter, knn_f1, s=18, alpha=0.7)\n",
    "ax.scatter(np.full_like(tree_f1, x_pos[1]) + (np.random.rand(len(tree_f1))-0.5)*x_jitter, tree_f1, s=18, alpha=0.7)\n",
    "ax.set_ylabel(\"Macro-F1\")\n",
    "ax.set_title(\"\")\n",
    "save_close(fig, \"boxplot_macroF1_kNN_vs_Tree.png\")\n",
    "\n",
    "fig = plt.figure(figsize=(6.0, 4.4))\n",
    "ax = plt.gca()\n",
    "fold_idx = knn_results.sort_values(\"fold\")[\"fold\"].to_numpy()\n",
    "ax.plot(fold_idx, knn_f1, marker=\"o\", label=\"k-NN\")\n",
    "ax.plot(fold_idx, tree_f1, marker=\"s\", label=\"Decision Tree\")\n",
    "for i in range(len(fold_idx)):\n",
    "    ax.plot([fold_idx[i], fold_idx[i]], [tree_f1[i], knn_f1[i]], alpha=0.5)  # connector\n",
    "ax.set_xlabel(\"Outer fold\")\n",
    "ax.set_ylabel(\"Macro-F1\")\n",
    "ax.set_title(\"\")\n",
    "ax.legend()\n",
    "save_close(fig, \"paired_macroF1_per_fold.png\")\n",
    "\n",
    "def row_normalise(df):\n",
    "    sums = df.sum(axis=1).replace(0, np.nan)\n",
    "    return df.div(sums, axis=0).fillna(0.0)\n",
    "\n",
    "def annotate_matrix(ax, mat, fmt=\"{:.1f}%\", cutoff=1.0):\n",
    "    \"\"\"\n",
    "    Write percentages into cells. 'cutoff' is percent threshold (e.g., 1.0 => only >=1% shown).\n",
    "    \"\"\"\n",
    "    nrows, ncols = mat.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            val = mat[i, j]*100.0\n",
    "            if val >= cutoff:\n",
    "                ax.text(j, i, fmt.format(val), ha=\"center\", va=\"center\", color=\"white\")\n",
    "\n",
    "def plot_confusion(cm_df, title, filename, labels, show_diag=True, cutoff=1.0):\n",
    "    \"\"\"\n",
    "    If show_diag=False, zero out the diagonal to emphasise misclassifications.\n",
    "    cutoff = percentage threshold below which annotations are hidden.\n",
    "    \"\"\"\n",
    "    cm_norm = row_normalise(cm_df).values.copy()\n",
    "    if not show_diag:\n",
    "        np.fill_diagonal(cm_norm, 0.0)\n",
    "\n",
    "    fig = plt.figure(figsize=(6.0, 5.2))\n",
    "    ax = plt.gca()\n",
    "    im = ax.imshow(cm_norm, aspect=\"auto\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted class\")\n",
    "    ax.set_ylabel(\"True class\")\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(labels)\n",
    "    cb = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cb.set_label(\"Row-normalised frequency\")\n",
    "    # Minor-grid to separate cells\n",
    "    ax.set_xticks(np.arange(-0.5, len(labels), 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, len(labels), 1), minor=True)\n",
    "    ax.grid(which=\"minor\", linestyle=\"-\", linewidth=0.25)\n",
    "\n",
    "    annotate_matrix(ax, cm_norm, cutoff=cutoff)\n",
    "    save_close(fig, filename)\n",
    "\n",
    "plot_confusion(knn_cm,\n",
    "               \"\",\n",
    "               \"confusion_kNN_rownorm_annotated.png\",\n",
    "               class_labels, show_diag=True, cutoff=1.0)\n",
    "\n",
    "plot_confusion(knn_cm,\n",
    "               \"\",\n",
    "               \"confusion_kNN_misclass_only.png\",\n",
    "               class_labels, show_diag=False, cutoff=0.5)\n",
    "\n",
    "plot_confusion(tree_cm,\n",
    "               \"\",\n",
    "               \"confusion_Tree_rownorm_annotated.png\",\n",
    "               class_labels, show_diag=True, cutoff=1.0)\n",
    "\n",
    "plot_confusion(tree_cm,\n",
    "               \"\",\n",
    "               \"confusion_Tree_misclass_only.png\",\n",
    "               class_labels, show_diag=False, cutoff=0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
